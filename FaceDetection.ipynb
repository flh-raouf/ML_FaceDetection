{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting labelme\n",
      "  Using cached labelme-5.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (2.16.1)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (3.8.4)\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-1.4.4-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting gdown (from labelme)\n",
      "  Using cached gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting imgviz>=1.7.5 (from labelme)\n",
      "  Using cached imgviz-1.7.5-py3-none-any.whl\n",
      "Collecting natsort>=7.1.0 (from labelme)\n",
      "  Using cached natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from labelme) (1.26.4)\n",
      "Collecting onnxruntime!=1.16.0,>=1.14.1 (from labelme)\n",
      "  Downloading onnxruntime-1.17.3-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: Pillow>=2.8 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from labelme) (10.3.0)\n",
      "Collecting PyYAML (from labelme)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting qtpy!=1.11.2 (from labelme)\n",
      "  Using cached QtPy-2.4.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting scikit-image (from labelme)\n",
      "  Downloading scikit_image-0.23.2-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from labelme) (2.4.0)\n",
      "Collecting PyQt5!=5.15.3,!=5.15.4 (from labelme)\n",
      "  Using cached PyQt5-5.15.10-cp37-abi3-win_amd64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from labelme) (0.4.6)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from albumentations) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from albumentations) (1.4.2)\n",
      "Collecting pydantic>=2.6.4 (from albumentations)\n",
      "  Downloading pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\n",
      "     ---------------------------------------- 0.0/103.4 kB ? eta -:--:--\n",
      "     ----------- --------------------------- 30.7/103.4 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 92.2/103.4 kB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ 103.4/103.4 kB 854.4 kB/s eta 0:00:00\n",
      "Collecting opencv-python-headless>=4.9.0 (from albumentations)\n",
      "  Using cached opencv_python_headless-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting coloredlogs (from onnxruntime!=1.16.0,>=1.14.1->labelme)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sympy (from onnxruntime!=1.16.0,>=1.14.1->labelme)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2.6.4->albumentations)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.1 (from pydantic>=2.6.4->albumentations)\n",
      "  Downloading pydantic_core-2.18.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting PyQt5-sip<13,>=12.13 (from PyQt5!=5.15.3,!=5.15.4->labelme)\n",
      "  Using cached PyQt5_sip-12.13.0-cp312-cp312-win_amd64.whl.metadata (524 bytes)\n",
      "Collecting PyQt5-Qt5>=5.15.2 (from PyQt5!=5.15.3,!=5.15.4->labelme)\n",
      "  Using cached PyQt5_Qt5-5.15.2-py3-none-win_amd64.whl.metadata (552 bytes)\n",
      "Collecting networkx>=2.8 (from scikit-image->labelme)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting imageio>=2.33 (from scikit-image->labelme)\n",
      "  Using cached imageio-2.34.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->labelme)\n",
      "  Downloading tifffile-2024.4.18-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->labelme)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from scikit-learn>=1.3.2->albumentations) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from scikit-learn>=1.3.2->albumentations) (3.4.0)\n",
      "Collecting beautifulsoup4 (from gdown->labelme)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from gdown->labelme)\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting tqdm (from gdown->labelme)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown->labelme)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.16.0,>=1.14.1->labelme)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown->labelme)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->onnxruntime!=1.16.0,>=1.14.1->labelme)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime!=1.16.0,>=1.14.1->labelme)\n",
      "  Using cached pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\storm tech\\desktop\\code\\mlbike-regression\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl (38.6 MB)\n",
      "Downloading albumentations-1.4.4-py3-none-any.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.4 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 112.6/150.4 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.4/150.4 kB 1.8 MB/s eta 0:00:00\n",
      "Using cached natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading onnxruntime-1.17.3-cp312-cp312-win_amd64.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB 7.9 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.4/5.6 MB 5.3 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.7/5.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.9/5.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.9/5.6 MB 4.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.2/5.6 MB 4.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.3/5.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.0/5.6 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.3/5.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.5/5.6 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.1/5.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.5/5.6 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.7/5.6 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.9/5.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.1/5.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.4/5.6 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.9/5.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.1/5.6 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.4/5.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 5.5 MB/s eta 0:00:00\n",
      "Using cached opencv_python_headless-4.9.0.80-cp37-abi3-win_amd64.whl (38.5 MB)\n",
      "Downloading pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "   ---------------------------------------- 0.0/407.9 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 337.9/407.9 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 407.9/407.9 kB 6.3 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.1-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/1.9 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/1.9 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/1.9 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 10.1 MB/s eta 0:00:00\n",
      "Using cached PyQt5-5.15.10-cp37-abi3-win_amd64.whl (6.8 MB)\n",
      "Using cached QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
      "Downloading scikit_image-0.23.2-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.5/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/12.8 MB 5.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.6/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.8/12.8 MB 5.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.9/12.8 MB 5.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.8 MB 5.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.5/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.6/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.8/12.8 MB 5.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.0/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.3/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.1/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.3/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.4/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.6/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.9/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.4/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.8/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.3/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.3/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.5/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.9/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.2/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.5/12.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.7/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.0/12.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.2/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.7/12.8 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.9/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.3/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.7/12.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.1/12.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.2/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.3/12.8 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.3/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.4/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.5/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.6/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.7/12.8 MB 5.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.8/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.8/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.8/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.8/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.8/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.4/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.9/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.4/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.4/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.7/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 4.6 MB/s eta 0:00:00\n",
      "Using cached gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached imageio-2.34.0-py3-none-any.whl (313 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.5/1.7 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.7 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.6/1.7 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.1/1.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 6.0 MB/s eta 0:00:00\n",
      "Using cached PyQt5_Qt5-5.15.2-py3-none-win_amd64.whl (50.1 MB)\n",
      "Using cached PyQt5_sip-12.13.0-cp312-cp312-win_amd64.whl (77 kB)\n",
      "Downloading tifffile-2024.4.18-py3-none-any.whl (224 kB)\n",
      "   ---------------------------------------- 0.0/225.0 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 204.8/225.0 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 225.0/225.0 kB 3.4 MB/s eta 0:00:00\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using cached pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "Installing collected packages: pyreadline3, PyQt5-Qt5, mpmath, tqdm, tifffile, sympy, soupsieve, qtpy, PyYAML, PySocks, PyQt5-sip, pydantic-core, opencv-python-headless, opencv-python, networkx, natsort, lazy-loader, imageio, humanfriendly, filelock, annotated-types, scikit-image, PyQt5, pydantic, coloredlogs, beautifulsoup4, onnxruntime, imgviz, gdown, albumentations, labelme\n",
      "Successfully installed PyQt5-5.15.10 PyQt5-Qt5-5.15.2 PyQt5-sip-12.13.0 PySocks-1.7.1 PyYAML-6.0.1 albumentations-1.4.4 annotated-types-0.6.0 beautifulsoup4-4.12.3 coloredlogs-15.0.1 filelock-3.13.4 gdown-5.1.0 humanfriendly-10.0 imageio-2.34.0 imgviz-1.7.5 labelme-5.4.1 lazy-loader-0.4 mpmath-1.3.0 natsort-8.4.0 networkx-3.3 onnxruntime-1.17.3 opencv-python-4.9.0.80 opencv-python-headless-4.9.0.80 pydantic-2.7.0 pydantic-core-2.18.1 pyreadline3-3.4.1 qtpy-2.4.1 scikit-image-0.23.2 soupsieve-2.5 sympy-1.12 tifffile-2024.4.18 tqdm-4.66.2\n"
     ]
    }
   ],
   "source": [
    "!pip install labelme tensorflow opencv-python matplotlib albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import albumentations as alb\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start taking images and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "IMAGES_PATH = os.path.join('data','images')\n",
    "number_images = 30 #You can modify how many images you want to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This cell will open your camera, capture pictures of you, and save them in the \"images\" folder inside the \"data\" directory.\n",
    "#If you wish to capture more pictures to expand your dataset, simply re-run this cell (100 captures should suffice).\n",
    "#Make sure to move across the frame, cover your face sometimes, make weird expressions, and exit the frame occasionally. All of this is necessary to obtain a good dataset.\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "for imgnum in range(number_images):\n",
    "    print('Collecting image {}'.format(imgnum))\n",
    "    ret, frame = cap.read()\n",
    "    imgname = os.path.join(IMAGES_PATH,f'{str(uuid.uuid1())}.jpg')\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw label rectangles in Images with LabelMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will open LabelMe, a software used to draw rectangles around your own face for labeling.\n",
    "# Once opened, click on \"Open Directory\" and select the directory containing the images (.\\data\\images).\n",
    "# Additionally, enable the \"Save Automatically\" option in the file panel.\n",
    "# In the same panel, click on \"Change Output Dir\" and set it to .\\data\\labels.\n",
    "# In the \"Edit\" menu, select \"Draw Rectangles\".\n",
    "# Now, simply draw a rectangle around the face, label it as 'face', and proceed to the next image.\n",
    "\n",
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Image into TF Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = tf.data.Dataset.list_files('data\\\\images\\\\*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(x): \n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Raw Images with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_generator = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_images = image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANUALLY SPLIT DATA INTO TRAIN TEST AND VAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the Matching Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for folder in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join('data', folder, 'images')):\n",
    "        \n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            new_filepath = os.path.join('data',folder,'labels',filename)\n",
    "            os.replace(existing_filepath, new_filepath)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Image Augmentation on Images and Labels using Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=450, height=450), \n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2), \n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Image with CV and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data','train', 'images','0aa64205-f2fb-11ee-951d-f9da159ac108.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'train', 'labels', '0aa64205-f2fb-11ee-951d-f9da159ac108.json'), 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Coordinates and Rescale to Match Image Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords = [0,0,0,0]\n",
    "coords[0] = label['shapes'][0]['points'][0][0]\n",
    "coords[1] = label['shapes'][0]['points'][0][1]\n",
    "coords[2] = label['shapes'][0]['points'][1][0]\n",
    "coords[3] = label['shapes'][0]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords = list(np.divide(coords, [640,480,640,480]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Augmentations and View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmented['bboxes'][0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'], \n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:], [450,450]).astype(int)), \n",
    "                    (255,0,0), 2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [640,480,640,480]))\n",
    "\n",
    "        try: \n",
    "            for x in range(120):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Augmented Images to Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Labels to Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Label and Image Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Partition Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Images and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx] \n",
    "    sample_coords = res[1][1][idx]\n",
    "\n",
    "    sample_image = cv2.UMat(sample_image)\n",
    "\n",
    "    cv2.rectangle(sample_image,\n",
    "                  tuple(np.multiply(sample_coords[:2], [120, 120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120, 120]).astype(int)),\n",
    "                  (255, 0, 0), 1)\n",
    "    \n",
    "    sample_image = sample_image.get()\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Deep Learning using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False) #idk what are theses commandes i just found them on the internet xdxdxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    input_layer = Input(shape=(120,120,3))\n",
    "    \n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    # Classification Model  \n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    # Bounding box model\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes, coords = facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Optimizer and LearningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1./0.75 -1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Localization Loss and Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):            \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] \n",
    "    w_true = y_true[:,2] - y_true[:,0] \n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out Loss Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "localization_loss(y[1], coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classloss(y[0], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressloss(y[1], coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Custom Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceTracker(Model): \n",
    "    def __init__(self, eyetracker,  **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.model = eyetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs): \n",
    "        \n",
    "        X, y = batch\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        y_class = tf.reshape(y[0], (-1, 1))\n",
    "        y_coords = tf.reshape(y[1], (-1, 4))\n",
    "\n",
    "\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "\n",
    "          \n",
    "            \n",
    "            batch_classloss = self.closs(y_class, classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y_coords, tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "\n",
    "        y_class = tf.reshape(y[0], (-1, 1))\n",
    "        y_coords = tf.reshape(y[1], (-1, 4))\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y_class, classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y_coords, tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)\n",
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=40, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#i had a problem with this : actually in the training it shows total_loss , class_loss..... but when i write hist.history it doesn't show them , so i skip this part of plotting performances , but i left it here (i didn't remove it) bcz i want an answer from you please\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions on Test Set (to verify accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "\n",
    "    sample_image = cv2.UMat(sample_image)\n",
    "    \n",
    "    if yhat[0][idx] > 0.9: \n",
    "\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 1)\n",
    "    \n",
    "    sample_image = sample_image.get()\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "facetracker.save('faceTrackerModel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "facetracker = load_model('facetrackermodel.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real Time Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret , frame = cap.read()\n",
    "    if not ret:  \n",
    "        break\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('faceTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
